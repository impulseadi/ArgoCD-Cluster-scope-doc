Kubeconfig and Private End Point Eks Cluster


Kubeconfig file:

A kubeconfig is a configuration file (YAML) that tells Kubernetes clients (for example kubectl or client libraries) which cluster to talk to, how to authenticate, and which user/context/namespace to use. kubectl reads this file to send requests to the API server.

The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.
Suppose you have several clusters, and your users and components authenticate in a variety of ways. For example:
●A running kubelet might authenticate using certificates.
●A user might authenticate using tokens.
●Administrators might have sets of certificates that they provide to individual users.
With kubeconfig files, you can organize your clusters, users, and namespaces. You can also define contexts to quickly and easily switch between clusters and namespaces.

Why do we need it:
●A Kubernetes cluster has an API server that accepts requests.
●Clients must know:
1.Where the API server is (server URL).
2.How to verify the server (CA certificate / TLS).
3.Who is making the request (credentials: token, cert, exec plugin, etc.).
4.Which namespace and user context to use by default.
The kubeconfig file stores all that information in one place so tools can operate without retyping flags every time.
Where it lives and how it’s chosen: 

Default location: ~/.kube/config (on Linux/macOS).
kubectl merges files listed in KUBECONFIG (later docs override earlier ones).
Protect this file: chmod 600 ~/.kube/config (it contains credentials).

A kubeconfig is YAML with four main sections:
●clusters — definitions of cluster endpoints and CA info.
●users (sometimes called “authinfos”) — credentials and auth method.
●contexts — named triples that bind a cluster + user + optional namespace.
●current-context — which context kubectl uses by default.
Example minimal kubeconfig:
apiVersion: v1
kind: Config
clusters:
- name: my-cluster
  cluster:
    server: https://34.12.56.78:6443
    certificate-authority-data: <base64-CA-cert>
users:
- name: my-user
  user:
    token: <bearer-token>            # OR client-certificate-data/client-key-data OR exec plugin
contexts:
- name: my-context
  context:
    cluster: my-cluster
    user: my-user
    namespace: dev
current-context: my-context

Kubeconfig commands:
kubectl config view                 # show merged kubeconfig (use --raw to see encoded values)
kubectl config view --minify        # show only the current context portion
kubectl config current-context      # show current context
kubectl config get-contexts         # list contexts
kubectl config use-context <name>   # switch context
kubectl config set-context <name> --cluster=<cluster> --user=<user> --namespace=<ns>
kubectl config set-credentials <user> --token=<token>             # add user/token
kubectl config set-cluster <cluster> --server=https://... --certificate-authority=ca.crt --embed-certs=true
kubectl --kubeconfig=/path/to/config get pods






To connect to private end point eks cluster:
EKS Cluster Access:
Every Kubernetes cluster has an API server (control plane) that clients (kubectl, apps) connect to.
In EKS, this API server can be exposed in two ways:
1.Public endpoint → Accessible over the internet.
2.Private endpoint → Accessible only from inside your VPC (AWS private network).
By default, when you create an EKS cluster, you choose if you want public, private, or both.
What is a Private Endpoint:
●A private endpoint means the EKS control plane API server is reachable only within the cluster’s VPC.
●No direct internet access to the API server.
●This is more secure, but it means you cannot just run kubectl from your laptop unless it has private network access.
Why to use Private Endpoint:
●Security → Reduces attack surface (API server not exposed on the internet).
●Compliance → Many companies require internal-only access.
●Controlled access → Only workloads inside the VPC or connected via VPN/Direct Connect can talk to the API server.
If your laptop is not inside the VPC (and you don’t have VPN/Direct Connect set up), you’ll see errors like:
Unable to connect to the server: dial tcp <IP>:443: i/o timeout




How to connect to a private EKS endpoint:
We need to give your machine private network access to the cluster’s VPC. Options:
Option A: Connect from inside the VPC
●EC2 Bastion Host:

○Launch an EC2 instance in the same VPC/subnet as your EKS cluster.

○SSH into it, install kubectl + awscli.
From that EC2, you can run:

 aws eks update-kubeconfig --region <region> --name <cluster-name>
kubectl get nodes
Acts as a “jump box”.
Option B: Use VPN
●Set up a VPN (AWS Site-to-Site VPN, Client VPN, or OpenVPN in the VPC).
●Once connected, your laptop is effectively inside the VPC, so you can use kubectl directly.

Option C: AWS Direct Connect
●If you have on-premises data centers, AWS Direct Connect links your network to the VPC privately.
●Then you can connect as if you’re inside the same network.

Option D: AWS VPC Peering / Transit Gateway
●If you already have another VPC where you work from, peer it with the EKS cluster VPC.
●Traffic flows privately → you can access the endpoint

Once you have network access (via bastion, VPN, etc.), you still need credentials.
Use AWS CLI to update your kubeconfig:

 aws eks update-kubeconfig --name <cluster-name> --region <region>
●This writes the cluster details + authentication plugin (aws-iam-authenticator via exec block) into your ~/.kube/config.
Then run:
 kubectl get ns
●If you’re inside the network → it will connect.


Step 1: Create EKS with private-only endpoint
If using eksctl:

 eksctl create cluster \
  --name my-private-cluster \
  --region us-east-1 \
  --vpc-private-subnets=subnet-abc,subnet-def \
  --without-nodegroup \
  --api-private-access \
  --api-public-access=false
●If using AWS Console: choose Private endpoint access only.



Step 2: Add a Bastion Host in the VPC
●Launch an EC2 instance in the same VPC/subnet.

Install AWS CLI + kubectl on it:

 sudo yum install -y awscli
curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/<VERSION>/2025-01-01/bin/linux/amd64/kubectl
chmod +x kubectl && sudo mv kubectl /usr/local/bin/
●Configure AWS credentials (IAM role or aws configure).


Step 3: Generate kubeconfig (inside bastion)
aws eks update-kubeconfig --name my-private-cluster --region us-east-1
kubectl get svc

This works because the EC2 is inside the VPC.

Step 4 (optional): Allow remote access from your laptop
If you don’t want to SSH into the bastion every time, you can:
Set up SSH port forwarding:

 ssh -i key.pem -L 6443:<private-api-endpoint>:443 ec2-user@<bastion-public-ip>
● Then point your kubeconfig server field to https://127.0.0.1:6443.

●Or, set up a VPN Client (AWS Client VPN, OpenVPN, etc.) so your laptop is “inside” the VPC.









